Work on this project done in May-August 2022  has been sequestered in a private repository.

_(Done on May 6-Aug 29, 2022; open sourced on Sep 16-17, 2022)_

I am making these materials available in this subdirectory (without reproducing the detailed commit timeline), and I'll continue this work elsewhere.

The most interesting breakthrough results were achieved on Jun 26-27, see below:

---

# 2022 phase of "arxiv-1606-09470-section3" project

This subdirectory is intended to serve as a **"lab journal"**, a detailed log of my
research efforts in connection with the _"arxiv-1606-09470-section3"_ project
in May-August 2022.

Efforts have been made to make this **"lab journal"** as reproducible as possible.
See [reproducibility.md](reproducibility.md) for the details of that.

## What and how

### What:

We are trying to synthesize a compact machine with certain behavior (the behavior is given by a set of training examples)

The success is measured by
  * how well the synthesized machine behaves outside of training data (on test examples)
  * whether the algorithm the synthesized machine encodes seems to make sense to a person reading it (those small models are quite readable)

### How:

For simplicity, the examples of behavior are generated by a handcrafted compact machine belonging to the same class of machines. This gives us a number of advantages:
  * it's easy to generate as many training and test data as one desires
  * it's great to know that at least one good solution to the problem in question does exist (namely, the handcrafted machine itself)
  * so it is reasonable to start with a relatively large non-specific machine made from a set of building blocks containing at least some number of copies of building blocks which have been used to actually create our handcrafted machine
  * and then we train our large model to reproduce the training data behavior and then we further apply "training with sparsification" to trim it down to a compact model
  * and then we can read the obtained model and mentally compare it to the one we had handcrafted, and based on that comparision we can pass judgement on whether the synthesized model is actually good (in addition to measuring its performance on a test set)

## History

The current version of the public DMM roadmap has been published in March 2021:
[Towards Practical Use of Dataflow Matrix Machines](https://www.cs.brandeis.edu/~bukatin/towards-practical-dmms.pdf)

Section A.2 (page 3) of that roadmap is calling for doing
_"circuit synthesis = synthesis of dataflow/functional reactive programs = DMM synthesis"_ via neural architecture search.

The _"arxiv-1606-09470-section3"_ project had been started
as an open source project in July 2021: [github:anhinga:julia-flux-drafts/arxiv-1606-09470-section3](https://github.com/anhinga/julia-flux-drafts/tree/main/arxiv-1606-09470-section3)

That project was aiming
to achive **"DMM synthesis via neural architecture search"** for a simple example by means
of _training with sparsifying regularization via novel differentiable
programming engines_ such as Julia Zygote or JAX. In August 2021, I had to
hit the _"pause"_ button because of the difficulties I was having with
Julia differentiable programming frameworks. In April 2022, the progress
in differentiable programming ecosystems and in my relationships with those
ecosystems reached the stage where this project could have been resumed.

The work in this subdirectory was done on May 6-Aug 29, 2022 and was open sourced on Sep 16-17, 2022.

---

## May 2022

This subdirectory was created on May 6. 

The initial development took place in the [rough-sketches](rough-sketches) subdirectory.

Differences between "DMM Lite" (here) and DMM 1.0 (2016-2017, https://github.com/jsa-aerial/DMM):

 * streams of flat dictionaries in DMM Lite instead of streams of trees (nested dictionaries) in DMM 1.0
 * the network matrix is outside the network in DMM Lite instead of being controlled by one of the neurons in DMM 1.0
 * fixed number of active neurons in DMM Lite instead of the number of active neurons being controlled by the network matrix in DMM 1.0
 * the network matrix is a tensor of rank 4 in DMM Lite vs tensor of rank 6 in DMM 1.0
 
On May 20 we reached the stage where the differentiable programming via `Zygote.jl` was
working for a handcrafted variadic DMM: [rough-sketches/variadic-dp-v0-0-1.jl](rough-sketches/variadic-dp-v0-0-1.jl)

On May 28-29 we reached the stage where DMM training pipeline worked for the first time
(the ADAM optimizer was rewritten for trees, and a trainable network was learning to match the output of 
a handcrafted network).

## June 2022

The first two weeks of June were spent trying various architectures for the networks to train
and sparsify (we were looking for a neural architecture we were ready to handle at the moment
in terms of convergence and convergence speeds, differentiable programming performance, and such).

In the second week of June, the development moved from the [rough-sketches](rough-sketches) subdirectory
to the [v0-1](v0-1) subdirectory **(v0.1)**, and the code underwent a series of refactorings; in particular
we stepped back from a fully recurrent setup for a while and started to use
_"feedforward transducers with locally recurrent elements"_ as the networks to train and sparsify.

The first truly successful training happened on June 19 at [v0-1/feedforward-run-3](v0-1/feedforward-run-3)
and the first truly successful sparsification experiment happened on June 26-27 at
[v0-1/feedforward-run-3/sparsification](v0-1/feedforward-run-3/sparsification).

One advantage of this approach was the resulting sparsified models were often compact enough to enable one
to read and fully understand them. Here we obtained a model with 19 soft and 3 hard links:
[v0-1/feedforward-run-3/sparsification/sparse20-after-100-steps-matrix.json](v0-1/feedforward-run-3/sparsification/sparse20-after-100-steps-matrix.json)

This should be compared to the handcrafted DMM with 11 links:
[v0-1/feedforward-run-3/sparsification/handcrafted.jl](v0-1/feedforward-run-3/sparsification/handcrafted.jl)

The model rediscovered the key motives of the handcrafted version and introduced some quirky innovations of its own.
We used extremely tiny training data, but the generalization within the training envelope was remarkably good:
[v0-1/feedforward-run-3/testing](v0-1/feedforward-run-3/testing)

---

## July-August 2022

We are currently in the transition, reflection, planning, and exploration period following this initial success.
This activity is happening along two axes:

  * Search for appropriate applications (circuit synthesis, AutoML, fast and scalable symbolic regression, etc.)
  * Going deeper in terms of automation, scaling, generalization, and understanding the properties of the training process and the resulting models

There has been three waves of experimental work during this period:

  * July 2-3: [v0-1/compare-free](v0-1/compare-free)
  * July 19-21: [v0-1/compare-free/sparsify](v0-1/compare-free/sparsify)
  * August 12-17: [v0-1/compare-free/sparsify2](v0-1/compare-free/sparsify2)
